#!/usr/bin/python

import sys, os

misc = {'classix':sys.argv[1]}      ## FOR TESTING
path = sys.argv[2]                  ## FOR TESTING
def openlog(a,b): return sys.stderr ## FOR TESTING
sid = 0                             ## FOR TESTING 

##sys.path.append("/home/coscowww/bcourse/bin")  ## FOR REAL
##from bcprelude import *                        ## FOR REAL
##from bc_cl_prelude import *                    ## FOR REAL
##from bcpictures import *                       ## FOR REAL

##form = cgi.FieldStorage()                      ## FOR REAL
##sid, path = getsid( form )                     ## FOR REAL
##misc = readmisc(path)                          ## FOR REAL

modpath = path + "/model"

classix = int(misc['classix'])

vdf  =    path + "/data.vd"
vdfa =    path + "/data.vd.allvars"
dspf =    path + "/data.dsp.selvars"
thtf = modpath + "/data.tht"

from string import *

def read_heads_and_tails(fn, hc=1):
    global g_hc, h
    g_hc = hc
    res = ()
    spllines = map( lambda x: split(x[:-1], '\t'), open(fn).readlines() )
    heads = [map( lambda x: x[h],   spllines ) for h in range(g_hc)]
    tails =  [map( lambda x: x[g_hc:], spllines )]
    return heads + tails

def quote(s):
    return '"'+s+'"'


def padempty(str, extra):
    if str == "": return extra
    else : return str

import re

int_re  = "-?\d+"
real_re = "-?(?:\d*)\.\d+"
num_re  = "((?:" + int_re + ")|(?:" + real_re +"))"

nummatch = re.compile('^' + num_re + '$')

def isnum(s) : return nummatch.match(s)
def isalf(s) : return not isnum(s)

thtlines = open(thtf).readlines()

nodenames_a, valuenames_a   = read_heads_and_tails(vdfa)
nodenames  , valuenames     = read_heads_and_tails(vdf)
asccounts, bincounts, divps = read_heads_and_tails(dspf,2) 

asccounts = map(int, asccounts);
bincounts = map(int, bincounts);

classcount = len(valuenames[classix])

paramcount = 0
offsets = []
nona = []
vana = []
r=0
i=0
var_ofs = []
var_probs = []
for k in range(classcount): var_probs.append([])

value_counts = []
value_name_offsets = []
value_names = []
value_name_counts = []

div_counts = []
div_offsets = []
divs = []

vno = 0
dvo = 0
var_types = ["E" for ei in range(len(nodenames_a))]

for n in nodenames:

	c = int(thtlines[r])
	a = nodenames_a.index(n)

	alflist = valuenames[i][bincounts[i]:]
	if bincounts[i] == 0: tp = "A"
	else:                 tp = "X"

	if i == classix :
		classindexdata = a
		r = r+1
		class_probs = split(thtlines[r][:-1], ' ')
		class_divs = divps[i]
		class_names = map(quote, valuenames[i])
		classvaluenamecount = len(alflist)
		class_value_names = map(quote, alflist)
		classdivcount = len(class_divs)
	elif c>1 :
		var_ofs.append(a)
		nona.append(nodenames[i])
		vana.append(valuenames[i])
		offsets.append(paramcount)
		paramcount = paramcount + len(valuenames[i])
		value_counts.append(len(valuenames[i]))
		for k in range(c): 
			r = r + 1
			var_probs[k].extend(split(thtlines[r][:-1], ' '))

		value_names.extend(map(quote, alflist))
		value_name_counts.append(len(alflist))
		value_name_offsets.append(vno)
		vno = vno + len(alflist)

		div_count = len(divps[i])
		divs.extend(divps[i])
		div_counts.append(div_count)
		div_offsets.append(dvo)
		dvo = dvo + div_count
	else :
		tp = 'E'
		for k in range(c): r = r + 1

	var_types[a] = tp

	r = r + 1
	i = i + 1

nodenames  = nona
valuenames = vana
nna = map(quote, nodenames_a)

datavarcount = len(nodenames_a)
modelvarcount = len(nodenames)

logf = openlog(sid, "a")
logf.write("\nC-code generated\n")


###################### PRINT THE PREDULE ###########################

print "Content-type: text/plain\n\n"
print """
/* This ANSI C program was automatically generated by B-Course. It is
   provided free for educational and teaching purposes, and it can be 
   freely modified. The program is provided without any guarantee of its
   suitability for anything. Using it assumes knowledge about compiling
   C-programs and running them.

   The purpose of this program is to classify data vectors. Data is
   supposed to be in the very same format, that was originally uploaded
   into B-Course, i.e. tab-limited, header row containing variable names
   and missing values coded as (non-tab) whitespace. The variables names
   have to match those in original data, but the contents of the actual
   data part are only checked for variables that were included in the 
   final model. For each data vector, the program prints out the most
   probable class and the predictive distribution. For those vectors that
   have class variable present, it also prints prediction accuracy.

   Input is read from standard input, output is printed to standard output
   and errors are reported to standard error, so that the normal way to 
   execute the program is (providing you have compiled it to nabe):

  	nabe  < testdata

   You are encouraged to change the source code to suit your needs.
   Especially, many errors can be turned into warnings and recovered by
   replacing data format violating parts of the data by MISSING_VALUE. 
   Hack fun.
*/

/* *********************** THE MODEL DESCRIPTION ************************* */
/*
   Just in case somebody is interested to look at the implementation
   here is some explanation about the data structures that constitute the 
   Naive Bayes classiffier model. First of all, there are three different
   types of variables: type E (for excluded) are those variables that are 
   in the data, but that are not used for the model (because of automatic
   or manual variable selection); type A (ascii) are those variables
   that are not E and that have been detected to have only non-numeric
   values (like "boy, "girl") in the data. The rest of the variables
   are of the type X i.e. they have numeric values, but might also have
   ascii values. 

   Since ISO C does not allow zero length arrays we define the following
   constants that should never be referred to, but are required in order to
   follow the C standard. These definitions make it easier to automatically 
   generate this file from the constant C-code part and per model 
   definitions. The actual values of these constants do not matter since
   they are not used in any computations.

*/

#define NEVER_USED_INDEX  (-1)
#define NEVER_USED_DIV    (-1.0)
#define NEVER_USED_COUNT  (-1)
#define NEVER_USED_NAME   ("")
#define NEVER_USED_PROB   (-1.0)

/*

   Naming of constants and data stuctures tries to follow some rules.
   In general we use the words DATA and MODEL to tell wether we refer to 
   all variables or just those used for the model. 

*/
"""

###################### PRINT THE THING ###########################

print """
/* Here is the information needed about all the variables in the data */

#define DATA_VAR_COUNT   (%d)
#define DATA_CLASS_INDEX (%d)

char  data_var_types[] = "%s";
char* data_var_names[] = {%s};
""" % (datavarcount, classindexdata, "".join(var_types), ", ".join(nna))



print """
/* From this on we only refer to the class variable and the predictor 
   variables (var) that are used for the model. The information for
   the class variable and predictors are stored in separate data structures.

   To avoid dynamic memory allocation, information about all the predictors 
   are often stored in a single table. Since different variables may have
   different number of values, we have to set up the system of offsets to 
   tell where in the table the information for a specific variable can be
   found. 
*/ """



print """
#define MODEL_VAR_COUNT     (%d)
int model_to_data_index[] = {%s};

#define CLASS_COUNT  (%d)
int value_counts[] = {%s};
""" % ( modelvarcount, 
	padempty(", ".join(map(str,var_ofs)), "NEVER_USED_INDEX"),
	classcount, 
	padempty(", ".join(map(str,value_counts)), "NEVER_USED_COUNT"))



print """
/* For the class variable and each predictor variable we describe: 

   a) How many ascii-values (called value names) they have, 
      and what they are. Since these value names are stored in a single
      table, we also have to define the offsets that tell where in the 
      table the value names for each variable are located. (Well, we
      could calculate this run time too using value_name counts, but ..)

   b) By how many division points (divs) the (possible) numerical values 
      of the variables are divided into intervals, and what are these
      division points. Again, since these divs are stored in a single
      table, we also have to define the offsets.
*/"""


print """
/* First for the class variable */
 
#define CLASS_VALUE_NAME_COUNT (%d)
char* class_value_names[]    = {%s};

#define CLASS_DIV_COUNT  (%d)
double class_divs[]    = {%s};
""" % ( classvaluenamecount,
	padempty(", ".join(class_value_names), "NEVER_USED_NAME"),
	classdivcount,
	padempty(", ".join(map(str,class_divs)), "NEVER_USED_DIV"))



print """
/* And then for predictor variables */

int value_name_counts[]  = {%s};
int value_name_offsets[] = {%s};
char* value_names[]      = {%s};

int div_counts[]  = {%s};
int div_offsets[] = {%s};
double divs[]     = {%s};
""" % ( padempty(", ".join(map(str,value_name_counts)), "NEVER_USED_COUNT"),
	padempty(", ".join(map(str,value_name_offsets)), "NEVER_USED_INDEX"),
	padempty(", ".join(value_names), "NEVER_USED_NAME"),
	padempty(", ".join(map(str,div_counts)), "NEVER_USED_COUNT"),
	padempty(", ".join(map(str,div_offsets)), "NEVER_USED_INDEX"),
	padempty(", ".join(map(str,divs)), "NEVER_USED_DIV")
        )



print """
/* The model consists about parameters that define probababilities.
   There are probabilities for each class and probabilities for each
   value of each variable in each different class. Since the parameters
   for all the variables are again in a single table, we have offset
   table for parameters.
*/ """

# vpl = map(lambda x: """{%s}""" % ", ".join(x), var_probs)
vpl = ["{" + padempty(", ".join(vps), "NEVER_USED_PROB") + "}"
       for vps in var_probs]

print """
#define PARAM_COUNT                           (%d)

float class_probs[CLASS_COUNT]              = {%s};
int param_offsets[/* MODEL_VAR_COUNT */]          = {%s};
float value_probs[CLASS_COUNT][/* PARAM_COUNT */ %d] = 
{
%s
};
""" % ( paramcount,
	", ".join(map(str,class_probs)),
	padempty(", ".join(map(str,offsets)), "NEVER_USED_INDEX"),
        max(paramcount,1),
	",\n\n".join(vpl))


print """
/* For reporting the numbering of classes */

char* class_names[] = {%s}; """% ", ".join(class_names)

print """
/* ******************* END OF THE MODEL DESCRIPTION ********************** */


/* Little something to configure. Turn them into zeros to disable.*/

#define HAS_HEADER_ROW                   (1)
#define PRINT_INDIVIDUAL_CLASSIFICATIONS (1)
#define PRINT_CONFUSION_MATRIX           (1)
#define PRINT_LOG_CONFUSION_MATRIX       (1)
#define PRINT_01_SCORE                   (1)
#define PRINT_LOG_SCORE                  (1)


/* For more confuguration, change the source code below */

#include <stdio.h>       /* For FILE, standard streams and sprintf */
#include <stdlib.h>      /* For atof                               */
#include <string.h>      /* For strcmp                             */ 
#include <ctype.h>       /* For isspace and isdigit                */
#include <float.h>       /* For DBL_MIN                            */
#include <math.h>        /* For log and exp                        */

#define CLASS_PROB(K)      (class_probs[K])
#define VALUE_PROB(K,I,L)  (value_probs[K][param_offsets[I] + (L)])
#define MODEL_VAR_NAME(I)  (data_var_names[model_to_data_index[I]])
#define CLASS_NAME         (data_var_names[DATA_CLASS_INDEX])

#define MAX_FIELD_LENGTH  (100)
#define ERROR_MSG_LENGTH  (MAX_FIELD_LENGTH * 3 + 200)


/*
   io_help wraps streams, buffers and variables needed for 
   input, output and error reporting. Especially it contains
   the buffer that is used for parsing the fields (i.e. strings
   between tabulators (and end of lines)) in input file.
*/

typedef struct io_help io_help;

struct io_help {
    FILE* data_in_fp;
    FILE* data_out_fp;
    FILE* data_err_fp;
    int c;                               /* For next input character    */
    int row;                             /* Input file row    1, 2, ... */
    int col;                             /* Input file column 1, 2, ... */
    char field_buffer[MAX_FIELD_LENGTH]; /* Buffer for parsing field    */
    int field_buffer_length;             /* Counter for field length    */
    char error_msg[ERROR_MSG_LENGTH];    /* Space for sprintfing errors */
};

static void 
init_io_help(io_help* iohp) {
    iohp->data_in_fp  = stdin;   /* These streams are now hardcoded. */
    iohp->data_out_fp = stdout;  /* Feel free to change or even      */
    iohp->data_err_fp = stderr;  /* parametrize this init function   */

    iohp->c = getc(stdin);       /* Always the next character to be processed */
    iohp->row = 1;               /* Row and col should always point to the    */
    iohp->col = 1;               /* place in file where c was read.           */
}



/* For now, this function is always called with do_exit != 0. However, it might
   be smarter to call sometimes with do_exit=0, and just give a warning. In this
   case one should handle the error somehow, usually by imputing data with
   missing values. NB! Initialising iohp->data_err_fp to NULL makes error 
   reporting mute.
*/

#define ERR_OK                                (0)
#define ERR_UNKNOWN_TYPE                      (1)
#define ERR_UNKNOWN_VALUE                     (2)
#define ERR_FIELD_TOO_LONG                    (3)
#define ERR_TOO_MANY_FIELDS                   (4)
#define ERR_NOT_ENOUGH_FIELDS                 (5)
#define ERR_UNKNOWN_VARIABLE_NAME             (6)
#define ERR_TOO_MANY_VARIABLE_NAMES           (7)
#define ERR_NOT_ENOUGH_VARIABLE_NAMES         (8)

static void 
report_error(io_help* iohp, int do_exit)
{
    int quiet = (iohp->data_err_fp == NULL);

    if(!quiet) {
	char* warror = do_exit ? "ERROR" : "WARNING";
	if(EOF == iohp->c){
	    fprintf(iohp->data_err_fp, "%s: after whole input read:\\n", warror);
	} else {
	    fprintf(iohp->data_err_fp, "%s: when at row %d, column %d:\\n",
		    warror, iohp->row, iohp->col);
	}
    
	fprintf(iohp->data_err_fp, "#### %s ####\\n\\n",iohp->error_msg);
    }
    
    if(do_exit) {
	if(!quiet) fprintf(iohp->data_err_fp, "Execution aborted.");
	exit(do_exit);
    }
}


/* For many lines now, we have code for turning the input line into
   array of integers, denoting the (discretized) values of included
   variables. Parsing the input takes at least 90% of the code - boooring.
*/

static int 
string_is_whitespace(char* string) 
{
    int c = 0;
    
    while(string[c] != 0 && isspace(string[c])) ++c;
    
    return string[c] == '\\0';
}

static int 
string_is_numeric(char* string) /* like -2.7 or .3 */
{
    int c = 0;
    int digit_found = 0;

    while(string[c] != 0 && isspace(string[c])) ++c;
    if (string[c] == 0) return 0;
    if   (string[c] != 0 && string[c] == '-'  ) ++c;
    while(string[c] != 0 && isdigit(string[c])) {++c; digit_found |= 1;}
    if   (string[c] != 0 && string[c] == '.'  ) ++c;
    while(string[c] != 0 && isdigit(string[c])) {++c; digit_found |= 1;}
    while(string[c] != 0 && isspace(string[c])) ++c;
    
    return (string[c] == 0) && digit_found;
}


/* Next four finds are simple linear searches. Strings are searched for exact
   match. I.e. differences in even leading and trailing whitespaces count. The
   numbers are searched for the (possible open) interval they belong to. 
   These methods return the index of match or enclosing interval. They are the
   core of turning values found in input file into integers denoting discretized
   values (0, 1, ..). 
*/

#define VALUE_NOT_FOUND   (-2)

static int 
find_class_for_number(float number)
{
    int val;

    for(val = 0; val<CLASS_DIV_COUNT; ++val){
	if(number <= class_divs[val]) return val; 
    }
    return val;
}

static int 
find_class_for_name(char* string) 
{
    int val;
    for(val = 0; val<CLASS_VALUE_NAME_COUNT; ++val){
	if(0 == strcmp(string, class_value_names[val])) return val;
    }
    return VALUE_NOT_FOUND;
}

static int 
find_value_for_number(float number, int i_m) 
{
    int val;

    for(val = 0; val<div_counts[i_m]; ++val){
	if(number <= divs[div_offsets[i_m] + val]) return val; 
    }
    return val;
}

static int 
find_value_for_name(char* string, int i_m) 
{
    int val;
    for(val = 0; val<value_counts[i_m]; ++val){
	if(0 == strcmp(string, value_names[value_name_offsets[i_m] + val])) 
	    return val;
    }
    return VALUE_NOT_FOUND;
}


/* The two functions below are practically identical. They implement
   string to int conversion for class variable and predictor variables
   respectively. 
 */

#define MISSING_VALUE     (-1)

static int 
find_class_value_for_string(char var_type, char* string, io_help* iohp)
{
    if(var_type == 'A') {
	return find_class_for_name(string);
    } else {
	if(string_is_numeric(string)) {
	    return find_class_for_number(atof(string));
	} else {
	    int name_index = find_class_for_name(string);
	    if(VALUE_NOT_FOUND != name_index) {
		return CLASS_DIV_COUNT + 1 + name_index;
	    } else {
		return VALUE_NOT_FOUND;
	    }
	}
    }
}

static int 
find_var_value_for_string(char var_type, int i_m, char* string, io_help* iohp)
{
    if(var_type == 'A') {
	return find_value_for_name(string, i_m);
    } else {
	if(string_is_numeric(string)) {
	    return find_value_for_number(atof(string), i_m);
	} else {
	    int name_index = find_value_for_name(string, i_m);
	    if(VALUE_NOT_FOUND != name_index) {
		return div_counts[i_m] + 1 + name_index;
	    } else {
		return VALUE_NOT_FOUND;
	    }
	}
    }
}


/* Just to call different functions for the class variable and the predictor
   variables.
 */

static int 
find_value_for_string (char var_type, int i_m, char* string, io_help* iohp) 
{
    int val = VALUE_NOT_FOUND;
    char* varname = "Internal Error";
    
    if(string_is_whitespace(string)) return MISSING_VALUE;
    
    if(i_m == -1) {
	varname = CLASS_NAME;
	val = find_class_value_for_string(var_type, string, iohp);
    } else {
	varname = MODEL_VAR_NAME(i_m);
	val = find_var_value_for_string(var_type, i_m, string, iohp);
    }
    
    if(val == VALUE_NOT_FOUND) {
	sprintf(iohp->error_msg, "Unknown value for variable %s (%s).",
		varname, string);
	report_error(iohp, ERR_UNKNOWN_VALUE);

	/* If you want to continue despite of errors you might do do by changing
	   report_error above to

	   report_error(iohp, ERR_OK); 
	   return MISSING_VALUE;
	*/
    }
    
    return val;
}


/* Now two low level functions for reading the input. set_new_line
   makes an attempt to take care of three different kinds of end of lines:
   "\\r" "\\n" and "\\r\\n". 
 */

static void 
read_next_char(io_help* iohp)
{
    iohp->c = getc(iohp->data_in_fp); 
    ++iohp->col;
}

static void 
set_new_line(io_help* iohp)
{
    if(iohp->c != EOF) {
	if(iohp->c == '\\r') read_next_char(iohp);
	if(iohp->c == '\\n') read_next_char(iohp);
    }
    ++iohp->row;
    iohp->col = 1; 
}


/* scan_field reads input to iohp->field_buffer and null-terminates it */

#define FIELD_ENDED (1)
#define LINE_ENDED  (2)

static int 
scan_field(io_help* iohp) 
{
    iohp->field_buffer_length = 0;
    
    while(1) {
	
	if(iohp->field_buffer_length == MAX_FIELD_LENGTH) {
	    sprintf(iohp->error_msg, "Field too long");
	    report_error(iohp, ERR_FIELD_TOO_LONG);

	    /* Alternatively you might skip to the next terminator,
	       set iohp->field_buffer to "" and return. Remember to
	       read the next character though. However, exceeding the
	       maximum field length is most probably an error, so it 
	       is better to abort.
	    */
	}
	
	switch (iohp->c) {
	case '\\t':
	    iohp->field_buffer[iohp->field_buffer_length] = 0;
	    read_next_char(iohp);
	    return FIELD_ENDED;
	case '\\r':
	case '\\n':
	    set_new_line(iohp);
	case EOF:
	    iohp->field_buffer[iohp->field_buffer_length] = 0;
	    return LINE_ENDED;
	default:
	    iohp->field_buffer[iohp->field_buffer_length++] = iohp->c;
	    read_next_char(iohp);
	}
    }
}


/* Main function to turn data line into the array of ints and an
   integer for the class */

static void 
read_line(io_help* iohp, int* datavector, int* true_class)
{
    int i_d = 0; /* Variable index in data */
    int i_m = 0; /* Variable index in model*/
    int scanning_status = FIELD_ENDED;
    
    for(i_d=0; scanning_status != LINE_ENDED; ++ i_d){

	char vt = data_var_types[i_d];

	scanning_status = scan_field(iohp);
	
	if(i_d < DATA_VAR_COUNT) {

	    if(vt == 'E') continue;

	    if(i_d == DATA_CLASS_INDEX) {
		*true_class 
		    = find_value_for_string(vt, -1, iohp->field_buffer, iohp);
	    } else {
		datavector[i_m] 
		    = find_value_for_string(vt, i_m, iohp->field_buffer, iohp);
		++ i_m;
	    }
	} else {
	    sprintf(iohp->error_msg, 
		    "Too many fields in data row "
		    "(expecting %d).",
		    DATA_VAR_COUNT);
	    report_error(iohp, ERR_TOO_MANY_FIELDS);
	}
    }
    
    if(i_d != DATA_VAR_COUNT){
	sprintf(iohp->error_msg, 
		"Not enough fields in data row "
		"(expecting %d, got %d).",
		DATA_VAR_COUNT, i_d);
	report_error(iohp, ERR_NOT_ENOUGH_FIELDS);
    }
}

static void 
read_header_row(io_help* iohp)
{
    int i_d = 0; /* Variable index in data */

    int scanning_status = FIELD_ENDED; /* i.e FALSE */
    
    for(i_d=0; scanning_status != LINE_ENDED; ++ i_d){
	
	scanning_status = scan_field(iohp);
	
	if(i_d < DATA_VAR_COUNT){
	    if(strcmp(data_var_names[i_d], iohp->field_buffer) != 0){
		sprintf(iohp->error_msg, 
			"Variable name number %d do not match. "
			"Training data had \\"%s\\", "
			"but this input has \\"%s\\".",
			i_d + 1, data_var_names[i_d], iohp->field_buffer);
		report_error(iohp, ERR_UNKNOWN_VARIABLE_NAME);

	    }
	} else {
	    sprintf(iohp->error_msg, 
		    "Too many fields in header row "
		    "(expecting %d).",
		    DATA_VAR_COUNT);
	    report_error(iohp, ERR_TOO_MANY_VARIABLE_NAMES);
	}
    }
    
    if(i_d != DATA_VAR_COUNT){
	sprintf(iohp->error_msg, 
		"Not enough fields in header row "
		"(expecting %d, got %d).",
		DATA_VAR_COUNT, i_d);
	    report_error(iohp, ERR_NOT_ENOUGH_VARIABLE_NAMES);
    }
}


/* Struct and functions for storing and printing classification results
 */

typedef struct results results;
struct results {
    int confusion[CLASS_COUNT][CLASS_COUNT];
    double log_confusion[CLASS_COUNT][CLASS_COUNT];
    int predicted_class_size[CLASS_COUNT];
    int true_class_size[CLASS_COUNT];
    int true_class_size_sum;
    double log_score;
    int score_01;
};

void init_results(results* pr)
{
    int c1, c2;
    for(c1=0; c1<CLASS_COUNT; ++c1){
	for(c2=0; c2<CLASS_COUNT; ++c2){
	    pr->log_confusion[c1][c2] = pr->confusion[c1][c2] = 0;
	}
	pr->predicted_class_size[c1] = pr->true_class_size[c1] = 0;
    }
    pr->true_class_size_sum = 0;
    pr->log_score = 0;
    pr->score_01 = 0;
}

void print_class_order(io_help* iohp)
{
    int c;
    fprintf(iohp->data_out_fp, 
	    "\\n### The classes are coded in the following order ###\\n\\n");
	
    fprintf(iohp->data_out_fp, "%s", class_names[0]);
    for(c=1; c<CLASS_COUNT; ++c){
	fprintf(iohp->data_out_fp, ", %s", class_names[c]);
    }
    fprintf(iohp->data_out_fp, "\\n");

}

void print_and_record_classification(int true_class, int most_probable_class, 
				     double* cdstr, results* pr, io_help* iohp)
{
    if (PRINT_INDIVIDUAL_CLASSIFICATIONS) {
	int c;
	int data_no = iohp->row - 1 - HAS_HEADER_ROW;

	char* ok = (true_class == MISSING_VALUE) ? "" 
	    : (true_class == most_probable_class) ? "Yes" : "No";
    

	if(data_no == 1){
	    fprintf(iohp->data_out_fp, 
		    "\\n"
		    "### Individual Classifications ###\\n"
		    "\\n" 
		    "Data    Ok   Correct Predicted   Predictive distribution\\n"
		    "========================================================\\n"
		    "\\n");
	}
	
	fprintf(iohp->data_out_fp, "%-5d  %3s   %-7d %-9d  ", 
		data_no, ok, true_class, most_probable_class);
	for(c=0; c<CLASS_COUNT; ++c) 
	    fprintf(iohp->data_out_fp, " %.3f", cdstr[c]);
	fprintf(iohp->data_out_fp, "\\n");
    }

    if(true_class != MISSING_VALUE){
	++ pr->confusion[true_class][most_probable_class];
	pr->log_confusion[true_class][most_probable_class] 
	    += log(cdstr[most_probable_class]);
	pr->log_score += log(cdstr[true_class]);
	if (true_class == most_probable_class) pr->score_01 += 1;
	++ pr->predicted_class_size[most_probable_class];
	++ pr->true_class_size[true_class];
	++ pr->true_class_size_sum;
    }
}


void print_results(results* pr, io_help* iohp)
{
    if(pr->true_class_size_sum == 0) return;
    
    if(PRINT_CONFUSION_MATRIX) {
	int c1, c2;
	fprintf(iohp->data_out_fp, 
		"\\n### Confusion matrix (rows are true classes,"
		" columns predicted) ###\\n\\n");
	for(c1=0; c1<CLASS_COUNT; ++c1){
	    for(c2=0; c2<CLASS_COUNT; ++c2){
		fprintf(iohp->data_out_fp, "%3d", pr->confusion[c1][c2]);
		fprintf(iohp->data_out_fp, "%c", 
			(c2 == CLASS_COUNT - 1)?'\\n':' ');
	    }
	}
    }

    if(PRINT_LOG_CONFUSION_MATRIX) {
	int c1, c2;

	fprintf(iohp->data_out_fp, "\\n");
	fprintf(iohp->data_out_fp, 
		"\\n### Probability confusion matrix ###\\n\\n");
    
	for(c1=0; c1<CLASS_COUNT; ++c1){
	    for(c2=0; c2<CLASS_COUNT; ++c2){
		if(pr->confusion[c1][c2] > 0){
		    fprintf(iohp->data_out_fp, "%.3f", 
			    exp(pr->log_confusion[c1][c2]/pr->confusion[c1][c2])
			    );
		} else {
		    fprintf(iohp->data_out_fp, "-----");
		}
		fprintf(iohp->data_out_fp, "%c", 
			(c2 == CLASS_COUNT - 1)?'\\n':' ');
	    }
	}
    }
    
    if(PRINT_LOG_SCORE || PRINT_01_SCORE) {
	fprintf(iohp->data_out_fp, "\\n");    
	fprintf(iohp->data_out_fp, "\\n### Summary ###\\n\\n");
    }

    
    if(PRINT_LOG_SCORE) {
	fprintf(iohp->data_out_fp, 
		"%d out of %d (%.1f%%) correctly classified.\\n",
		pr->score_01, pr->true_class_size_sum, 
		100.0*pr->score_01/pr->true_class_size_sum);
    }

    if(PRINT_01_SCORE) {
	fprintf(iohp->data_out_fp,
		"On (geometric) average, " 
		"the correct class gets probability %.3f.\\n\\n",
		exp(pr->log_score/pr->true_class_size_sum));
    }
}


/* The following function is the Naive Bayes Classifier, there is not much to it
   but it often works pretty well. Note the on-demand normalization,
   that protects for underflows.
*/

#define NORMALIZING_LIMIT (DBL_MIN*10000)

void classify_datarow(int* datavector, int true_class, 
		      results* pr, io_help* iohp)
{
    double cdstr[CLASS_COUNT];
    double normalizer = 0;
    int most_probable_class = -1;
    int k, i;
    
    for(k=0; k<CLASS_COUNT; ++k) normalizer += cdstr[k] = CLASS_PROB(k);
    
    for(i=0;i<MODEL_VAR_COUNT;++i) {
	int needs_normalizing = 0;
	if(datavector[i] == MISSING_VALUE) continue;
	
	normalizer = 0;
	for(k=0; k<CLASS_COUNT; ++k){
	    normalizer += cdstr[k] *= VALUE_PROB(k,i,datavector[i]);
	    if (cdstr[k] < NORMALIZING_LIMIT) needs_normalizing = 1;
	}
	
	if(needs_normalizing) {
	    for(k=0; k<CLASS_COUNT; ++k) {
		cdstr[k] /= normalizer;
	    }
	    normalizer = 1;
	}
    }
    
    for(k=0; k<CLASS_COUNT; ++k) {
	cdstr[k] /= normalizer;
	if(k==0 || cdstr[k]>cdstr[most_probable_class]) {
	    most_probable_class = k;
	}
    }
    
    print_and_record_classification(true_class, most_probable_class, 
				    cdstr, pr, iohp);
}


/* Now the top level functions. Nothing much to comment */

void read_and_classify_datarow(io_help* iohp, results* pr, int* datavector)
{
    int true_class = -5;
    read_line(iohp, datavector, &true_class);
    classify_datarow(datavector, true_class, pr, iohp);
}

void read_and_classify_datarows() 
{
    io_help ioh;
    results pr;
    int datavector[MODEL_VAR_COUNT];
    
    init_io_help(&ioh);
    init_results(&pr);
    
    if(HAS_HEADER_ROW) read_header_row(&ioh);

    if(PRINT_CONFUSION_MATRIX || PRINT_LOG_CONFUSION_MATRIX 
	|| PRINT_INDIVIDUAL_CLASSIFICATIONS) {
	print_class_order(&ioh);
    }

    while(ioh.c != EOF) {
	read_and_classify_datarow(&ioh, &pr, datavector);
    }
    
    print_results(&pr, &ioh);
}

void usage(int argc, char* argv[])
{
    fprintf(stderr, "Usage: %s\\n",argv[0]);
    fprintf(stderr, 
	    "\\t%s reads data from stdin and prints results to stdout.\\n", 
	    argv[0]);
}

int main (int argc, char* argv[])
{
    if(argc != 1){
	usage(argc, argv);
	exit(1);
    }
    read_and_classify_datarows();
    
    return 0;
}

/*
    ;;; Local Variables: ***
    ;;; mode:c ***
    ;;; c-basic-offset:4 ***
    ;;; End: ***
*/

"""
